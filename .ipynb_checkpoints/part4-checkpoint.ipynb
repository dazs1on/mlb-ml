{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "461446d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "\n",
    "# these are functions related to parsing the baseball reference page\n",
    "\n",
    "def get_game_summary(soup, game_id):\n",
    "    game = {'game_id': game_id}\n",
    "    scorebox = soup.find('div', {'class':'scorebox'})\n",
    "    teams = scorebox.findAll('a',{'itemprop':'name'})\n",
    "    game['away_team_abbr'] = teams[0]['href'].split('/')[2]\n",
    "    game['home_team_abbr'] = teams[1]['href'].split('/')[2]\n",
    "    meta = scorebox.find('div', {'class':'scorebox_meta'}).findAll('div')\n",
    "    game['date'] = meta[0].text.strip()\n",
    "    game['start_time'] = meta[1].text[12:-6].strip()\n",
    "    return game\n",
    "\n",
    "def get_table_summary(soup, table_no):\n",
    "    stats_tables = soup.findAll('table', {'class':'stats_table'})\n",
    "    t = stats_tables[table_no].find('tfoot')\n",
    "    summary = {x['data-stat']:x.text.strip() for x in t.findAll('td')}\n",
    "    return summary\n",
    "\n",
    "def get_pitcher_data(soup, table_no):\n",
    "    stats_tables = soup.findAll('table', {'class':'stats_table'})\n",
    "    t = stats_tables[table_no]\n",
    "    data = []\n",
    "    rows = t.findAll('tr')[1:-1] # not the header and footer rows\n",
    "    for r in rows:\n",
    "        summary = {x['data-stat']:x.text.strip() for x in r.findAll('td')}\n",
    "        summary['name'] = r.find('th',{'data-stat':'player'}).find('a')['href'].split('/')[-1][:-6].strip()\n",
    "        data.append(summary)\n",
    "    return data\n",
    "\n",
    "def process_link(url):\n",
    "    resp = requests.get(url)\n",
    "    game_id = url.split('/')[-1][:-6]\n",
    "\n",
    "    # strange preprocessing routine\n",
    "    uncommented_html = ''\n",
    "    for h in resp.text.split('\\n'):\n",
    "        if '<!--     <div' in h: continue\n",
    "        if h.strip() == '<!--': continue\n",
    "        if h.strip() == '-->': continue\n",
    "        uncommented_html += h + '\\n'\n",
    "\n",
    "    soup = bs(uncommented_html)\n",
    "    data = {\n",
    "        'game': get_game_summary(soup, game_id),\n",
    "        'away_batting': get_table_summary(soup, 1),\n",
    "        'home_batting':get_table_summary(soup, 2),\n",
    "        'away_pitching':get_table_summary(soup, 3),\n",
    "        'home_pitching':get_table_summary(soup, 4),\n",
    "        'away_pitchers': get_pitcher_data(soup, 3),\n",
    "        'home_pitchers': get_pitcher_data(soup, 4)\n",
    "    }\n",
    "    return data\n",
    "\n",
    "def get_covers_data(date_string):\n",
    "    odds_data = []\n",
    "    # get the web page with game data on it\n",
    "    url = f'https://www.covers.com/Sports/MLB/Matchups?selectedDate={date_string}'\n",
    "    resp = requests.get(url)\n",
    "\n",
    "    # parse the games\n",
    "    scraped_games = bs(resp.text).findAll('div',{'class':'cmg_matchup_game_box'})\n",
    "    for g in scraped_games:\n",
    "        game = {}\n",
    "        game['home_moneyline'] = g['data-game-odd']\n",
    "        game['date'] = g['data-game-date']\n",
    "        game['away_team_abbr'] = g['data-away-team-shortname-search']\n",
    "        game['home_team_abbr'] = g['data-home-team-shortname-search']\n",
    "        try:\n",
    "            game['home_score'] =g.find('div',{'class':'cmg_matchup_list_score_home'}).text.strip()\n",
    "            game['away_score'] =g.find('div',{'class':'cmg_matchup_list_score_away'}).text.strip()\n",
    "        except:\n",
    "            game['home_score'] =''\n",
    "            game['away_score'] =''\n",
    "\n",
    "        odds_data.append(game)\n",
    "    return odds_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8717bbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "game_data = pickle.load(open('game_data.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73dbc10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New games downloaded:  1303\n"
     ]
    }
   ],
   "source": [
    "current_year=2021\n",
    "\n",
    "# find all games for the year\n",
    "url = f\"https://www.baseball-reference.com/leagues/MLB/{current_year}-schedule.shtml\"\n",
    "resp = requests.get(url)\n",
    "soup=bs(resp.text)\n",
    "game_soups = soup.findAll('a',text='Boxscore')\n",
    "game_links = [x['href'] for x in game_soups] # for instance '/boxes/LAN/LAN202007230.shtml'\n",
    "\n",
    "# compare against downloaded games\n",
    "downloaded_games = [g['game']['game_id'] for g in game_data]\n",
    "new_game_links = [x for x in game_links if x[-18:-6] not in downloaded_games]\n",
    "\n",
    "# get the new games\n",
    "for link in new_game_links:\n",
    "    url = 'https://www.baseball-reference.com' + link\n",
    "    game_data.append(process_link(url))\n",
    "print(\"New games downloaded: \", len(new_game_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5a246f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datetime in c:\\users\\aaron\\anaconda3\\lib\\site-packages (4.3)\n",
      "Requirement already satisfied: pytz in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from datetime) (2021.1)\n",
      "Requirement already satisfied: zope.interface in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from datetime) (5.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from zope.interface->datetime) (52.0.0.post20210125)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88ddfbd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 Games today\n",
      "TBA202107100 1:10PM TOR stripro01 TBR yarbrry01\n",
      "MIN202107100 2:10PM DET  MIN \n",
      "BAL202107100 4:05PM CHW giolilu01 BAL eshelto01\n",
      "SFN202107100 4:05PM WSN lestejo01 SFG desclan01\n",
      "TEX202107100 4:05PM OAK kaprija01 TEX foltymi01\n",
      "BOS202107100 4:10PM PHI moorema02 BOS perezma02\n",
      "MIA202107100 4:10PM ATL friedma01 MIA rogertr01\n",
      "NYN202107101 4:10PM PIT anderty01 NYM stromma01\n",
      "CLE202107100 6:10PM KCR minormi01 CLE quantca01\n",
      "CHN202107100 7:15PM STL kimkw01 CHC davieza02\n",
      "HOU202107100 7:15PM NYY colege01 HOU greinza01\n",
      "MIL202107100 7:15PM CIN gutievl01 MIL peralfr01\n",
      "LAN202107100 10:10PM ARI smithca03 LAD buehlwa01\n",
      "SDN202107100 10:10PM COL marquge01 SDP musgrjo01\n",
      "SEA202107100 10:10PM LAA sandopa02 SEA flexech01\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "today_games=[]\n",
    "url = 'https://www.baseball-reference.com/previews/'\n",
    "page = requests.get(url).text\n",
    "soup = bs(page)\n",
    "summaries = soup.findAll('div', {'class':'game_summary'})\n",
    "for s in summaries:\n",
    "    game = {\n",
    "        'game':{\n",
    "            'game_id': s.find('a', text='Preview')['href'][-18:-6],\n",
    "            'is_test':True,\n",
    "            'date': dt.datetime.now().strftime('%A, %B %d, %Y')\n",
    "        },\n",
    "        'home_batting':{'R':0},\n",
    "        'away_batting':{'R':0},\n",
    "        'home_pitching':{'R':0},\n",
    "        'away_pitching':{'R':0},\n",
    "        'home_pitchers':[{'R':0}],\n",
    "        'away_pitchers':[{'R':0}]\n",
    "    }\n",
    "    cells = s.findAll('table')[0].findAll('td')\n",
    "\n",
    "    # skip postponed games\n",
    "    if not s.find('a', text=\"Preview\"): continue\n",
    "\n",
    "    # skip game 2 in double header - links look like this for 2nd games: \"/previews/2020/PHI202008051.shtml\"\n",
    "    if s.find('a', text=\"Preview\")['href'][-7]=='2': continue\n",
    "\n",
    "    try:\n",
    "        team_links = s.findAll('a')\n",
    "        game['game']['away_team_abbr'] = team_links[0]['href'].split('/')[2]\n",
    "        game['game']['home_team_abbr'] = team_links[2]['href'].split('/')[2]\n",
    "    except Exception as e:\n",
    "        #just all star games trigger this, I think\n",
    "        print(team_links)\n",
    "        continue\n",
    "\n",
    "    # get time\n",
    "    game['game']['start_time'] = s.find('table',{'class':'teams'}).find('tbody').findAll('tr')[1].findAll('td')[2].text.strip()\n",
    "    # get pitchers\n",
    "    try:\n",
    "        cells = s.findAll('table')[1].findAll('td')\n",
    "        game['away_pitchers'][0]['name'] = cells[1].find('a')['href'].split('/')[-1][:-6].strip()\n",
    "        game['home_pitchers'][0]['name'] = cells[3].find('a')['href'].split('/')[-1][:-6].strip()\n",
    "    except Exception as e:\n",
    "        # no pitcher\n",
    "        game['away_pitchers'][0]['name'] = ''\n",
    "        game['home_pitchers'][0]['name'] = ''\n",
    "    today_games.append(game)\n",
    "game_data.extend(today_games)\n",
    "print(len(today_games), \"Games today\")\n",
    "for x in today_games: print(x['game']['game_id'], x['game']['start_time'],\n",
    "                            x['game']['away_team_abbr'],x['away_pitchers'][0]['name'],\n",
    "                            x['game']['home_team_abbr'],x['home_pitchers'][0]['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1105fd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created game_df, batting_df, pitching_df and pitcher_df\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "games = []\n",
    "batting = []\n",
    "pitching = []\n",
    "pitchers = []\n",
    "\n",
    "for g in game_data:\n",
    "    game_summary = g['game']\n",
    "    if 'is_test' not in game_summary.keys(): game_summary['is_test']=False\n",
    "    # fix date\n",
    "    game_summary['date'] = game_summary['date'] + \" \" + game_summary['start_time']\n",
    "    del game_summary['start_time']\n",
    "\n",
    "    # get starting pitchers\n",
    "    game_summary['home_pitcher'] = g['home_pitchers'][0]['name']\n",
    "    game_summary['away_pitcher'] = g['away_pitchers'][0]['name']\n",
    "\n",
    "    # this is the field we'll train our model to predict\n",
    "    game_summary['home_team_win'] = int(g['home_batting']['R'])>int(g['away_batting']['R'])\n",
    "    games.append(game_summary)\n",
    "\n",
    "    # add all stats to appropriate lists\n",
    "    target_pairs = [\n",
    "        ('away_batting', batting),\n",
    "        ('home_batting', batting),\n",
    "        ('away_pitching', pitching),\n",
    "        ('home_pitching', pitching),\n",
    "        ('away_pitchers', pitchers),\n",
    "        ('home_pitchers', pitchers)\n",
    "    ]\n",
    "    for key, d in target_pairs:\n",
    "        if isinstance(g[key], list): # pitchers\n",
    "            for x in g[key]:\n",
    "                if 'home' in key:\n",
    "                    x['is_home_team'] = True\n",
    "                    x['team'] = g['game']['home_team_abbr']\n",
    "                else:\n",
    "                    x['is_home_team'] = False\n",
    "                    x['team'] = g['game']['away_team_abbr']\n",
    "                x['game_id'] = g['game']['game_id']\n",
    "                d.append(x)\n",
    "        else: #batting, pitching\n",
    "            x = g[key]\n",
    "            if 'home' in key:\n",
    "                x['is_home_team'] = True\n",
    "                x['team'] = g['game']['home_team_abbr']\n",
    "                x['spread'] = int(g[key]['R']) - int(g[key.replace('home','away')]['R'])\n",
    "            else:\n",
    "                x['is_home_team'] = False\n",
    "                x['team'] = g['game']['away_team_abbr']\n",
    "                x['spread'] = int(g[key]['R']) - int(g[key.replace('away','home')]['R'])\n",
    "            x['game_id'] = g['game']['game_id']\n",
    "            d.append(x)\n",
    "\n",
    "game_df = pd.DataFrame(games)\n",
    "game_df['date'] = pd.to_datetime(game_df['date'], errors='coerce')\n",
    "game_df = game_df[~game_df['game_id'].str.contains('allstar')].copy() #don't care about allstar games\n",
    "\n",
    "batting_df = pd.DataFrame(batting)\n",
    "for k in batting_df.keys():\n",
    "    if any(x in k for x in ['team','game_id', 'home_away']): continue\n",
    "    batting_df[k] =pd.to_numeric(batting_df[k],errors='coerce', downcast='float')\n",
    "batting_df.drop(columns=['details'], inplace=True)\n",
    "\n",
    "pitching_df = pd.DataFrame(pitching)\n",
    "for k in pitching_df.keys():\n",
    "    if any(x in k for x in ['team','game_id', 'home_away']): continue\n",
    "    pitching_df[k] =pd.to_numeric(pitching_df[k],errors='coerce', downcast='float')\n",
    "pitcher_df = pd.DataFrame(pitchers)\n",
    "\n",
    "for k in pitcher_df.keys():\n",
    "    if any(x in k for x in ['team','name','game_id', 'home_away']): continue\n",
    "    pitcher_df[k] =pd.to_numeric(pitcher_df[k],errors='coerce', downcast='float')\n",
    "# filter the pitcher performances to just the starting pitcher\n",
    "pitcher_df = pitcher_df[~pitcher_df['game_score'].isna()].copy().reset_index(drop=True)\n",
    "pitcher_df.drop(columns=[x for x in pitcher_df.keys() if 'inherited' in x], inplace=True)\n",
    "\n",
    "print(\"Created game_df, batting_df, pitching_df and pitcher_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c9c327f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aaron\\anaconda3\\lib\\site-packages\\pandas\\core\\window\\rolling.py:455: RuntimeWarning: All-NaN slice encountered\n",
      "  return func(x, start, end, min_periods)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of our main dataframe is now (rows x columns): (10019, 1127)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def add_rolling(period, df, stat_columns):\n",
    "    for s in stat_columns:\n",
    "        if 'object' in str(df[s].dtype): continue\n",
    "        df[s+'_'+str(period)+'_Avg'] = df.groupby('team')[s].apply(lambda x:x.rolling(period).mean())\n",
    "        df[s+'_'+str(period)+'_Std'] = df.groupby('team')[s].apply(lambda x:x.rolling(period).std())\n",
    "        df[s+'_'+str(period)+'_Skew'] = df.groupby('team')[s].apply(lambda x:x.rolling(period).skew())\n",
    "    return df\n",
    "\n",
    "def get_diff_df(df, name, is_pitcher=False):\n",
    "    #runs for each of the stat dataframes, returns the difference in stats\n",
    "\n",
    "    #set up dataframe with time index\n",
    "    df['date'] = pd.to_datetime(df['game_id'].str[3:-1], format=\"%Y%m%d\")\n",
    "    df = df.sort_values(by='date').copy()\n",
    "    newindex = df.groupby('date')['date']\\\n",
    "             .apply(lambda x: x + np.arange(x.size).astype(np.timedelta64))\n",
    "    df = df.set_index(newindex).sort_index()\n",
    "\n",
    "    # get stat columns\n",
    "    stat_cols = [x for x in df.columns if 'int' in str(df[x].dtype)]\n",
    "    stat_cols.extend([x for x in df.columns if 'float' in str(df[x].dtype)])\n",
    "\n",
    "    #add lags\n",
    "    df = add_rolling('5d', df, stat_cols) # this game series\n",
    "    df = add_rolling('10d', df, stat_cols)\n",
    "    df = add_rolling('45d', df, stat_cols)\n",
    "    df = add_rolling('180d', df, stat_cols) # this season\n",
    "    df = add_rolling('730d', df, stat_cols) # 2 years\n",
    "\n",
    "    # reset stat columns to just the lags (removing the original stats)\n",
    "    df.drop(columns=stat_cols, inplace=True)\n",
    "    stat_cols = [x for x in df.columns if 'int' in str(df[x].dtype)]\n",
    "    stat_cols.extend([x for x in df.columns if 'float' in str(df[x].dtype)])\n",
    "\n",
    "    # shift results so that each row is  a pregame stat\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = df.sort_values(by='date')\n",
    "    for s in stat_cols:\n",
    "        if is_pitcher:\n",
    "            df[s] = df.groupby('name')[s].shift(1)\n",
    "        else:\n",
    "            df[s] = df.groupby('team')[s].shift(1)\n",
    "\n",
    "    # calculate differences in pregame stats from home vs. away teams\n",
    "    away_df = df[~df['is_home_team']].copy()\n",
    "    away_df = away_df.set_index('game_id')\n",
    "    away_df = away_df[stat_cols]\n",
    "\n",
    "    home_df = df[df['is_home_team']].copy()\n",
    "    home_df = home_df.set_index('game_id')\n",
    "    home_df = home_df[stat_cols]\n",
    "\n",
    "    diff_df = home_df.subtract(away_df, fill_value=0)\n",
    "    diff_df = diff_df.reset_index()\n",
    "\n",
    "    # clean column names\n",
    "    for s in stat_cols:\n",
    "        diff_df[name + \"_\" + s] = diff_df[s]\n",
    "        diff_df.drop(columns=s, inplace=True)\n",
    "\n",
    "    return diff_df\n",
    "\n",
    "df = game_df\n",
    "df = pd.merge(left=df, right = get_diff_df(batting_df, 'batting'),\n",
    "               on = 'game_id', how='left')\n",
    "df = pd.merge(left=df, right = get_diff_df(pitching_df, 'pitching'),\n",
    "               on = 'game_id', how='left')\n",
    "df = pd.merge(left=df, right = get_diff_df(pitcher_df, 'pitcher',is_pitcher=True),\n",
    "               on = 'game_id', how='left')\n",
    "\n",
    "#pitcher rest feature\n",
    "pitcher_df = pd.DataFrame(pitchers) # old version was filtered to just starters\n",
    "dates = pitcher_df['game_id'].str[3:-1]\n",
    "pitcher_df['date'] = pd.to_datetime(dates,format='%Y%m%d', errors='coerce')\n",
    "pitcher_df['rest'] = pitcher_df.groupby('name')['date'].diff().dt.days\n",
    "# filter the pitcher performances to just the starting pitcher\n",
    "pitcher_df = pitcher_df[~pitcher_df['game_score'].isna()].copy().reset_index(drop=True)\n",
    "home_pitchers = pitcher_df[pitcher_df['is_home_team']].copy().reset_index(drop=True)\n",
    "df = pd.merge(left=df, right=home_pitchers[['game_id','name', 'rest']],\n",
    "              left_on=['game_id','home_pitcher'],\n",
    "              right_on=['game_id','name'],\n",
    "              how='left')\n",
    "df.rename(columns={'rest':'home_pitcher_rest'}, inplace=True)\n",
    "away_pitchers = pitcher_df[~pitcher_df['is_home_team']].copy().reset_index(drop=True)\n",
    "df = pd.merge(left=df, right=away_pitchers[['game_id','name','rest']],\n",
    "              left_on=['game_id','away_pitcher'],\n",
    "              right_on=['game_id','name'],\n",
    "              how='left')\n",
    "df.rename(columns={'rest':'away_pitcher_rest'}, inplace=True)\n",
    "df['rest_diff'] = df['home_pitcher_rest']-df['away_pitcher_rest']\n",
    "\n",
    "#datetime features\n",
    "df.dropna(subset=['date'], inplace=True)\n",
    "df['season'] = df['date'].dt.year\n",
    "df['month']=df['date'].dt.month\n",
    "df['week']=df['date'].dt.isocalendar().week.astype('int')\n",
    "df['dow']=df['date'].dt.weekday\n",
    "df['date'] = (pd.to_datetime(df['date']) - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1s') #epoch time\n",
    "\n",
    "print(\"The shape of our main dataframe is now (rows x columns):\",df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3e5ef6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of our main dataframe after adding skill rankings is (rows x columns): (10019, 1143)\n"
     ]
    }
   ],
   "source": [
    "from elote import EloCompetitor\n",
    "ratings = {}\n",
    "for x in df.home_team_abbr.unique():\n",
    "    ratings[x]=EloCompetitor()\n",
    "for x in df.away_team_abbr.unique():\n",
    "    ratings[x]=EloCompetitor()\n",
    "\n",
    "home_team_elo = []\n",
    "away_team_elo = []\n",
    "elo_exp = []\n",
    "\n",
    "df = df.sort_values(by='date').reset_index(drop=True)\n",
    "for i, r in df.iterrows():\n",
    "    # get pre-game ratings\n",
    "    elo_exp.append(ratings[r.home_team_abbr].expected_score(ratings[r.away_team_abbr]))\n",
    "    home_team_elo.append(ratings[r.home_team_abbr].rating)\n",
    "    away_team_elo.append(ratings[r.away_team_abbr].rating)\n",
    "    # update ratings\n",
    "    if r.home_team_win:\n",
    "        ratings[r.home_team_abbr].beat(ratings[r.away_team_abbr])\n",
    "    else:\n",
    "        ratings[r.away_team_abbr].beat(ratings[r.home_team_abbr])\n",
    "\n",
    "df['elo_exp'] = elo_exp\n",
    "df['home_team_elo'] = home_team_elo\n",
    "df['away_team_elo'] = away_team_elo\n",
    "\n",
    "#elo slow\n",
    "ratings = {}\n",
    "for x in df.home_team_abbr.unique():\n",
    "    ratings[x]=EloCompetitor()\n",
    "    ratings[x]._k_score=16\n",
    "for x in df.away_team_abbr.unique():\n",
    "    ratings[x]=EloCompetitor()\n",
    "    ratings[x]._k_score=16\n",
    "\n",
    "home_team_elo = []\n",
    "away_team_elo = []\n",
    "elo_exp = []\n",
    "\n",
    "df = df.sort_values(by='date').reset_index(drop=True)\n",
    "for i, r in df.iterrows():\n",
    "    # get pregame ratings\n",
    "    elo_exp.append(ratings[r.home_team_abbr].expected_score(ratings[r.away_team_abbr]))\n",
    "    home_team_elo.append(ratings[r.home_team_abbr].rating)\n",
    "    away_team_elo.append(ratings[r.away_team_abbr].rating)\n",
    "    # update ratings\n",
    "    if r.home_team_win:\n",
    "        ratings[r.home_team_abbr].beat(ratings[r.away_team_abbr])\n",
    "    else:\n",
    "        ratings[r.away_team_abbr].beat(ratings[r.home_team_abbr])\n",
    "\n",
    "df['elo_slow_exp'] = elo_exp\n",
    "df['home_team_elo_slow'] = home_team_elo\n",
    "df['away_team_elo_slow'] = away_team_elo\n",
    "\n",
    "#glicko\n",
    "from elote import GlickoCompetitor\n",
    "ratings = {}\n",
    "for x in df.home_team_abbr.unique():\n",
    "    ratings[x]=GlickoCompetitor()\n",
    "for x in df.away_team_abbr.unique():\n",
    "    ratings[x]=GlickoCompetitor()\n",
    "\n",
    "home_team_glick = []\n",
    "away_team_glick = []\n",
    "glick_exp = []\n",
    "\n",
    "df = df.sort_values(by='date').reset_index(drop=True)\n",
    "for i, r in df.iterrows():\n",
    "    # get pregame ratings\n",
    "    glick_exp.append(ratings[r.home_team_abbr].expected_score(ratings[r.away_team_abbr]))\n",
    "    home_team_glick.append(ratings[r.home_team_abbr].rating)\n",
    "    away_team_glick.append(ratings[r.away_team_abbr].rating)\n",
    "    # update ratings\n",
    "    if r.home_team_win:\n",
    "        ratings[r.home_team_abbr].beat(ratings[r.away_team_abbr])\n",
    "    else:\n",
    "        ratings[r.away_team_abbr].beat(ratings[r.home_team_abbr])\n",
    "\n",
    "df['glick_exp'] = glick_exp\n",
    "df['home_team_glick'] = home_team_glick\n",
    "df['away_team_glick'] = away_team_glick\n",
    "\n",
    "#trueskill\n",
    "from trueskill import Rating, quality, rate\n",
    "ratings = {}\n",
    "for x in df.home_team_abbr.unique():\n",
    "    ratings[x]=Rating(25)\n",
    "for x in df.away_team_abbr.unique():\n",
    "    ratings[x]=Rating(25)\n",
    "for x in df.home_pitcher.unique():\n",
    "    ratings[x]=Rating(25)\n",
    "for x in df.away_pitcher.unique():\n",
    "    ratings[x]=Rating(25)\n",
    "\n",
    "ts_quality = []\n",
    "pitcher_ts_diff = []\n",
    "team_ts_diff = []\n",
    "home_pitcher_ts = []\n",
    "away_pitcher_ts = []\n",
    "home_team_ts = []\n",
    "away_team_ts = []\n",
    "df = df.sort_values(by='date').copy()\n",
    "for i, r in df.iterrows():\n",
    "    # get pre-match trueskill ratings from dict\n",
    "    match = [(ratings[r.home_team_abbr], ratings[r.home_pitcher]),\n",
    "            (ratings[r.away_team_abbr], ratings[r.away_pitcher])]\n",
    "    ts_quality.append(quality(match))\n",
    "    pitcher_ts_diff.append(ratings[r.home_pitcher].mu-ratings[r.away_pitcher].mu)\n",
    "    team_ts_diff.append(ratings[r.home_team_abbr].mu-ratings[r.away_team_abbr].mu)\n",
    "    home_pitcher_ts.append(ratings[r.home_pitcher].mu)\n",
    "    away_pitcher_ts.append(ratings[r.away_pitcher].mu)\n",
    "    home_team_ts.append(ratings[r.home_team_abbr].mu)\n",
    "    away_team_ts.append(ratings[r.away_team_abbr].mu)\n",
    "\n",
    "    if r.date < df.date.max():\n",
    "        # update ratings dictionary with post-match ratings\n",
    "        if r.home_team_win==1:\n",
    "            match = [(ratings[r.home_team_abbr], ratings[r.home_pitcher]),\n",
    "                     (ratings[r.away_team_abbr], ratings[r.away_pitcher])]\n",
    "            [(ratings[r.home_team_abbr], ratings[r.home_pitcher]),\n",
    "            (ratings[r.away_team_abbr], ratings[r.away_pitcher])] = rate(match)\n",
    "        else:\n",
    "            match = [(ratings[r.away_team_abbr], ratings[r.away_pitcher]),\n",
    "                     (ratings[r.home_team_abbr], ratings[r.home_pitcher])]\n",
    "            [(ratings[r.away_team_abbr], ratings[r.away_pitcher]),\n",
    "            (ratings[r.home_team_abbr], ratings[r.home_pitcher])] = rate(match)\n",
    "\n",
    "df['ts_game_quality'] = ts_quality\n",
    "df['pitcher_ts_diff'] = pitcher_ts_diff\n",
    "df['team_ts_diff'] = team_ts_diff\n",
    "df['home_pitcher_ts'] = home_pitcher_ts\n",
    "df['away_pitcher_ts'] = away_pitcher_ts\n",
    "df['home_team_ts'] = home_team_ts\n",
    "df['away_team_ts'] = away_team_ts\n",
    "\n",
    "print(\"The shape of our main dataframe after adding skill rankings is (rows x columns):\",df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48468826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "odds_data = pickle.load(open('covers_data.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "62462cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Days of odds downloaded: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "dates = pd.to_datetime(df['date'], unit='s', errors='coerce')\n",
    "game_days = dates.dt.strftime('%Y-%m-%d').unique()\n",
    "existing_odds_days = [x['date'][:10] for x in odds_data]\n",
    "new_game_days = [x for x in game_days if x not in existing_odds_days]\n",
    "\n",
    "for d in new_game_days:\n",
    "    # get the web page with game data on it\n",
    "    url = f'https://www.covers.com/Sports/MLB/Matchups?selectedDate={d}'\n",
    "    resp = requests.get(url)\n",
    "\n",
    "    # parse the games\n",
    "    scraped_games = bs(resp.text).findAll('div',{'class':'cmg_matchup_game_box'})\n",
    "    for g in scraped_games:\n",
    "        game = {}\n",
    "        game['home_moneyline'] = g['data-game-odd']\n",
    "        game['date'] = g['data-game-date']\n",
    "        game['away_team_abbr'] = g['data-away-team-shortname-search']\n",
    "        game['home_team_abbr'] = g['data-home-team-shortname-search']\n",
    "        try:\n",
    "            game['home_score'] =g.find('div',{'class':'cmg_matchup_list_score_home'}).text.strip()\n",
    "            game['away_score'] =g.find('div',{'class':'cmg_matchup_list_score_away'}).text.strip()\n",
    "        except:\n",
    "            game['home_score'] =''\n",
    "            game['away_score'] =''\n",
    "\n",
    "        odds_data.append(game)\n",
    "print(\"Done! Days of odds downloaded:\", len(new_game_days))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7fc83a08",
   "metadata": {},
   "outputs": [
    {
     "ename": "MergeError",
     "evalue": "incompatible merge keys [2] dtype('int64') and dtype('float64'), must be the same type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMergeError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-edd4c7211b0a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;31m# do the merge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m df = pd.merge_asof(left=df.sort_values(by='date'),\n\u001b[0m\u001b[0;32m     34\u001b[0m                    \u001b[0mright\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0modds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'home_team_abbr'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'date'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'away_team_abbr'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'odds_proba'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'date'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m                    \u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'home_team_abbr'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'away_team_abbr'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36mmerge_asof\u001b[1;34m(left, right, on, left_on, right_on, left_index, right_index, by, left_by, right_by, suffixes, tolerance, allow_exact_matches, direction)\u001b[0m\n\u001b[0;32m    549\u001b[0m     \u001b[1;36m4\u001b[0m \u001b[1;36m2016\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m25\u001b[0m \u001b[1;36m13\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m00.048\u001b[0m   \u001b[0mAAPL\u001b[0m   \u001b[1;36m98.00\u001b[0m       \u001b[1;36m100\u001b[0m     \u001b[0mNaN\u001b[0m     \u001b[0mNaN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m     \"\"\"\n\u001b[1;32m--> 551\u001b[1;33m     op = _AsOfMerge(\n\u001b[0m\u001b[0;32m    552\u001b[0m         \u001b[0mleft\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m         \u001b[0mright\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, left, right, on, left_on, right_on, left_index, right_index, by, left_by, right_by, axis, suffixes, copy, fill_method, how, tolerance, allow_exact_matches, direction)\u001b[0m\n\u001b[0;32m   1662\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdirection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1663\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1664\u001b[1;33m         _OrderedMerge.__init__(\n\u001b[0m\u001b[0;32m   1665\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1666\u001b[0m             \u001b[0mleft\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, left, right, on, left_on, right_on, left_index, right_index, axis, suffixes, copy, fill_method, how)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1557\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill_method\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfill_method\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1558\u001b[1;33m         _MergeOperation.__init__(\n\u001b[0m\u001b[0;32m   1559\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1560\u001b[0m             \u001b[0mleft\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    666\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    667\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 668\u001b[1;33m         ) = self._get_merge_keys()\n\u001b[0m\u001b[0;32m    669\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m         \u001b[1;31m# validate the merge keys dtypes. We may need to coerce\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m_get_merge_keys\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1747\u001b[0m                         \u001b[1;34mf\"{repr(rk.dtype)}, must be the same type\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1748\u001b[0m                     )\n\u001b[1;32m-> 1749\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mMergeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1750\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1751\u001b[0m         \u001b[1;31m# validate tolerance; datetime.timedelta or Timedelta if we have a DTI\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMergeError\u001b[0m: incompatible merge keys [2] dtype('int64') and dtype('float64'), must be the same type"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "odds = pd.DataFrame(odds_data)\n",
    "odds['home_moneyline'].replace('', np.nan, inplace=True)\n",
    "odds.dropna(subset=['home_moneyline'], inplace=True)\n",
    "odds.home_moneyline = pd.to_numeric(odds.home_moneyline)\n",
    "odds.date = pd.to_datetime(odds.date).dt.date\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "odds.home_team_abbr[odds.home_team_abbr=='SF']='SFG'\n",
    "odds.home_team_abbr[odds.home_team_abbr=='TB']='TBR'\n",
    "odds.home_team_abbr[odds.home_team_abbr=='WAS']='WSN'\n",
    "odds.home_team_abbr[odds.home_team_abbr=='KC']='KCR'\n",
    "odds.home_team_abbr[odds.home_team_abbr=='SD']='SDP'\n",
    "\n",
    "odds.away_team_abbr[odds.away_team_abbr=='SF']='SFG'\n",
    "odds.away_team_abbr[odds.away_team_abbr=='TB']='TBR'\n",
    "odds.away_team_abbr[odds.away_team_abbr=='WAS']='WSN'\n",
    "odds.away_team_abbr[odds.away_team_abbr=='KC']='KCR'\n",
    "odds.away_team_abbr[odds.away_team_abbr=='SD']='SDP'\n",
    "\n",
    "odds['odds_proba']=np.nan\n",
    "odds['odds_proba'][odds.home_moneyline<0] = -odds.home_moneyline/(-odds.home_moneyline + 100)\n",
    "odds['odds_proba'][odds.home_moneyline>0] = (100/(odds.home_moneyline + 100))\n",
    "\n",
    "# get dates into the same format\n",
    "odds['date'] = (pd.to_datetime(pd.to_datetime(odds['date'])) - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1s')\n",
    "\n",
    "# do the merge\n",
    "df = pd.merge_asof(left=df.sort_values(by='date'),\n",
    "                   right=odds[['home_team_abbr','date', 'away_team_abbr','odds_proba']].sort_values(by='date'),\n",
    "                   by=['home_team_abbr','away_team_abbr'],\n",
    "                   on='date')\n",
    "df = df.sort_values(by='date').copy().reset_index(drop=True)\n",
    "print('Dataframe shape after adding odds data:', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5014cbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ece2931",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
